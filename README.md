# Reinforcement Learning From Human Feedback

For more information, visit [Reinforcement Learning From Human Feedback](https://www.deeplearning.ai/short-courses/reinforcement-learning-from-human-feedback/).

<p align="center">
  <img src="https://github.com/RomanRosa/Reinforcement-Learning-From-Human-Feedback/blob/main/Reinforcement%20Learning%20From%20Human%20Feedback.png">
</p>

## Skills and Knowledge Acquired

### Reinforcement Learning from Human Feedback (RLHF)

- 🧠 **Gained Conceptual Understanding of RLHF**: Acquired a conceptual understanding of Reinforcement Learning from Human Feedback (RLHF), including the datasets required for this technique.

### Fine-Tuning the Llama 2 Model

- 🛠️ **Fine-Tuned the Llama 2 Model with RLHF**: Successfully fine-tuned the Llama 2 model using RLHF with the open-source Google Cloud Pipeline Components Library.

### Model Performance Evaluation

- 🔍 **Evaluated Tuned Model Performance**: Compared the tuned model's performance against the base model using various evaluation methods.

## Insights Gained from the RLHF Training Course

### Overview of Large Language Models (LLMs) and RLHF

In this course, I deepened my understanding of how Large Language Models (LLMs) are trained on human-generated text and the necessity of additional methods like Reinforcement Learning from Human Feedback (RLHF) to align an LLM with human values and preferences. RLHF is particularly useful for tuning a base LLM to align with specific values and preferences for various use cases.

### Core Learning Components

- 📚 **Exploring RLHF Training Datasets**: I explored the two datasets used in RLHF training - the “preference” and “prompt” datasets.

- 🛠️ **Fine-Tuning Llama 2 Model with RLHF**: Utilizing the open-source Google Cloud Pipeline Components Library, I fine-tuned the Llama 2 model with RLHF.

- 🔍 **Assessing Tuned LLM Performance**: I assessed the tuned LLM against the original base model by comparing loss curves and employing the “Side-by-Side (SxS)” evaluation method.

This course provided me with a conceptual understanding of the RLHF training process and practical experience in applying RLHF to tune an LLM.
